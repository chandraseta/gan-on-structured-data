{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Udacity Deep Learning course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "<center>$S_{y_i} = \\frac{e^{y_i}}{\\sum_{j}^{}e^{y_j}}$</center>\n",
    "\n",
    "**Softmax** or **Normalized Exponential Function** is used to represent probabilistic distribution of K-dimensional vector that sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8360188  0.11314284 0.05083836]\n",
      "(1, 80)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Softmax.\"\"\"\n",
    "\n",
    "scores = [3.0, 1.0, 0.2]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "print(softmax(scores))\n",
    "\n",
    "# Plot softmax curves\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-2.0, 6.0, 0.1)\n",
    "scores = np.vstack([x])\n",
    "\n",
    "print(scores.shape)\n",
    "\n",
    "plt.plot(x, softmax(scores).T, linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Ones and Zeros only.\n",
    "\n",
    "Might increase accuracy, but highly increase computing costs (all those zeros) when the number of feature in the category increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "<center>$D(S,L) = - \\sum_{i}L_{i}\\log(S_{i})$</center>\n",
    "\n",
    "S = Softmax Distribution\n",
    "\n",
    "L = One-Hot Encoding Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss\n",
    "\n",
    "<center>$loss = \\frac{1}{N}\\sum_{i}D(S(wx_{i} + b), L_{i})$</center>\n",
    "\n",
    "Training loss is the average cross-entropy, and is defined by the value of **w** (weights) and **b** (biases). Since we want the least amount of loss, we need to minimize the value by finding the optimal w and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is used to find the optimal values of w and b, but it is not very effective if the dimension of data is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 30 Example Rule\n",
    "\n",
    "If a change affects at least 30 data to be classified correctly, then it could be classified as a meaningful change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "Activation function is an important feature in a neural network. It decides when a neuron should be activated (respond or ignore the given information). Activation function is the **non-linear transformation** that is done to the input signal. The transformed output is then sent to the next layer of neurons as input.\n",
    "\n",
    "<center>$Y = Activation(\\Sigma(weight * input) + bias)$</center>\n",
    "\n",
    "A neural network **without** an activation function is basically a **linear regression** model. A linear model simply could not solve more complex tasks that a non-linear model could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Step\n",
    "\n",
    "A threshold-based classifier, if the input value is above a given threshold, then the neuron is activated, else it will be deactivated.\n",
    "\n",
    "<center> $f(x) = 1, x >= 0$ </center>\n",
    "\n",
    "Unfortunately, it is only applicable for a **binary classifier**. The step function is not really useful for multi-class classifier, since the gradient is 0. During back-propagation, the gradient of the activation function is used in error calculations, so a gradient of 0 is not really useful at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Function\n",
    "\n",
    "<center>$f(x) = ax + b$</center>\n",
    "\n",
    "A basic linear equation that causes the activation to be proportional to the input. For multi-class classifier, we can simply choose the one with the highest value.\n",
    "\n",
    "<center>$f'(x) = a$</center>\n",
    "\n",
    "However, the derivative of a linear function is constant; it is independent of the input value $x$. This causes a problem during back-propagation since the gradient is pretty much the same and we did not actually improve our errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "\n",
    "<center>$f(x) = \\frac{1}{1 + e^{-x}}$</center>\n",
    "\n",
    "A sigmoid is a smooth function that is continuously differentiable. The value ranges between 0 to 1, with high gradient between -3 and 3. This means small change in range (-3,3) will cause large changes in the output value.\n",
    "\n",
    "The function essentially tries to push Y values toward the extremes, and that is good for a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "\n",
    "<center>$f(x) = \\frac{2}{1 + e^{-2x}}-1$</center>\n",
    "\n",
    "A tanh is very similar to the sigmoid function but is symmetric over the origin point. The value ranges between 0 to 1, with high gradient between -2 and 2.\n",
    "\n",
    "The function has steeper gradient compared to the sigmoid function. Choosing between sigmoid and tanh depends on the gradient requirement in the problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit (ReLU)\n",
    "\n",
    "<center>$f(x) = max(0,x)$</center>\n",
    "\n",
    "ReLU is one of the most widely used activation function today. It is non-linear, which means back-propagation is viable. Furthermore, ReLU does not activate all neurons at the same time, negative input will convert to zero and the neuron does not get activated. This means only a few neurons are activated, making the network sparse and faster for computation.\n",
    "\n",
    "Unfortunately, ReLU could create dead neurons which never get activated because the gradient on the negative side of the graph is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU\n",
    "\n",
    "<center>$ f(x) = ax, x < 0 $</center>\n",
    "\n",
    "<center>$ f(x) = x, x \\geqslant 0 $</center>\n",
    "\n",
    "An improved version of ReLU, where the horizontal line of 0 for negative values is replaced with a non-horizontal line with a non-zero gradient. This solves the dead neurons problem that a traditional ReLU has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "<center>$S_{y_i} = \\frac{e^{y_i}}{\\sum_{j}^{}e^{y_j}}$</center>\n",
    "\n",
    "Softmax is a type of sigmoid function that is able to handle **more than two classes**. The softmax function creates probabilities for each class based on the input value. It is ideally used in the output layer of the classifier where we need to get the probabilities to define the class of each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Activation Function\n",
    "\n",
    "- Sigmoid and Tanh functions are sometimes avoided due to **vanishing gradient problem**\n",
    "- ReLU and its derivatives are often used in most cases\n",
    "- Leaky ReLU eliminates dead neurons problem that regular ReLU has\n",
    "- ReLU should **only be used in hidden layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward and Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n",
    "<center>$v = \\displaystyle\\sum_{i}input_{i}*weight_{i}$</center>\n",
    "\n",
    "<center>$y = \\varphi(v)$</center>\n",
    "\n",
    "With $\\varphi$ as the activation function, y is the output of each node, which is also the input for nodes in the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "\n",
    "**Local gradient for output unit**\n",
    "\n",
    "<center>$\\delta_{k} = o_{k}(1-o_{k})(t_{k} - o_{k})$</center>\n",
    "\n",
    "Where $o$ denotes _output value_, and $t$ denotes _target value_.\n",
    "\n",
    "**Local gradient for hidden unit**\n",
    "\n",
    "<center>$\\delta_{h} = o_{h}(1-o_{h})(\\displaystyle\\sum_{k \\epsilon nextlayer}w_{h,k}\\delta_{k})$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Weight with momentum\n",
    "\n",
    "<center>$w(n+1) = w(n) + \\alpha * w(n-1) + \\eta * \\delta(n) * y$</center>\n",
    "\n",
    "Where $\\alpha$ denotes the momentum and $\\eta$ denotes the learning rate.\n",
    "\n",
    "Don't forget to update weight for **bias**! (Same formula, replace $w$ with $b$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways for initializing the weights in a neural network:\n",
    "- Set all weights to 0\n",
    "- Set all weights by drawing from standard normal distribution with mean=0 and variance=1\n",
    "\n",
    "Setting all the weights to 0 basically turns the model to a **linear model** because all the weights will be the same values in the iterations.\n",
    "\n",
    "<center>$ W^{[l]} = np.random.randn(size^{[l]}, size^{[l-1]})$</center>\n",
    "\n",
    "<center>Python's implementation using numpy, where $size^{[l]}$ represents the number of nodes in layer $l$</center>\n",
    "\n",
    "Drawing from standard normal distribution is obviously better for a deep network. However, using that particular method would introduce 2 issues:\n",
    "- **Vanishing Gradients** -- where the gradient will become smaller and smaller at every iteration, preventing the weights from updating their values.\n",
    "- **Exploding Gradients** -- the opposite of vanishing gradients, where the gradient gets bigger along the layers in an iteration, this may cause the model to oscillate around the minima and fail to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He\n",
    "\n",
    "<center> $\\sqrt{\\frac{2}{size^{[l-1]}}}$ </center>\n",
    "\n",
    "Commonly used for ReLU, the drawn values from standard normal distribution is multiplied by the expression above. While this does not completely remove the vanishing and exploding gradient issues, it helps to mitigate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Glorot\n",
    "\n",
    "<center> $\\sqrt{\\frac{1}{size^{[l-1]}}}$ </center>\n",
    "\n",
    "Commonly used for Tanh, similar to He but the numerator is 1 instead of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout basically forces all neurons in a layer to learn by randomly deactivating some of them in every iteration. This could help prevent overfitting as, hopefully, every neurons in a layer learn the same \"concept\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "- $w$ weight\n",
    "- $L$ loss function\n",
    "- $\\alpha$ learning rate\n",
    "- $v$ momentum\n",
    "- $\\mu$ momentum rate\n",
    "- $m$ moment\n",
    "- $g$ second order moment\n",
    "- $\\gamma$ decay rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Generate an _estimate_ of training loss based on **random** subset of data. This would minimize the cost of each iteration, since only a subset of data is processed. However, if the picked subset is not random enough, SGD might fail because the estimation is not general enough.\n",
    "\n",
    "<center>$w_{t+1} = w_{t} - \\alpha \\delta L(w_{t})$</center>\n",
    "\n",
    "SGD takes much smaller steps in each iteration and does more iteration than usual gradient descent. In the end, though, SGD is much more efficient and scales well with both data and model size.\n",
    "\n",
    "Tips to improve SGD:\n",
    "- **Inputs** should have equal (small) variance with mean = 0.\n",
    "- **Initial Weights** should be **random**, have equal (small variance) and mean = 0.\n",
    "\n",
    "#### SGD + Momentum\n",
    "\n",
    "<center>$v_{t+1} = \\mu v_{t} - \\alpha \\delta L(w_{t})$</center>\n",
    "\n",
    "<center>$w_{t+1} = w_{t} + v_{t+1}$</center>\n",
    "\n",
    "SGD takes very small steps toward the minimum loss function, and using **momentum** could help it converges faster. Momentum keeps track of previous changes ($\\Delta w$)\n",
    "\n",
    "Just like the momentum in classical physics, momentum helps accelerate the gradient at the right direction. This reduces the amount of oscillation and also helps in escaping local minima.\n",
    "\n",
    "#### SGD + Learning Rate Decay\n",
    "\n",
    "The steps taken in SGD should gradually be smaller to be more accurate. To do that, a learning rate decay is needed. There are a lot of decay algorithms out there, such as exponential decay algorithm. Defining the best decay algorithm is a whole new research area, but generally, the learning rate should decrease as the SGD gets closer to the minima.\n",
    "\n",
    "P.S.: A higher learning rate does not necessarily mean a better end-result (smaller loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "Scales learning rate according to the history of gradients by dividing current gradient in update rule by the sum of previous gradients.\n",
    "\n",
    "<center> $g_{t+1} = g_{t} + \\delta L(w_{t})^2$ </center>\n",
    "\n",
    "<center>$w_{t+1} = w_{t} - \\frac{\\alpha \\delta L(w_{t})^2}{\\sqrt{g_{t+1}} + \\epsilon}$</center>\n",
    "\n",
    "However, using AdaGrad could lead to a problem where the accumulated gradient will become very large and the learning rate becomes very small, causing the model to effectively stop learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "Root Mean Square Propagation tries to overcome the accumulated gradient problem in AdaGrad by applying a decay rate $\\gamma$. The term $g$ (second order moment) is calculated by exponentially decaying average and not the sum of gradients.\n",
    "\n",
    "\n",
    "Calculating first order moment,\n",
    "\n",
    "<center> $m_{t+1} = \\gamma m_{t} + (1-\\gamma)\\delta L(w_{t})$ </center>\n",
    "\n",
    "\n",
    "Calculating second order moment,\n",
    "\n",
    "<center> $g_{t+1} = \\gamma g_{t} + (1-\\gamma)\\delta L(w_{t})^2$ </center>\n",
    "\n",
    "\n",
    "Calculating momentum,\n",
    "\n",
    "<center> $v_{t+1} = \\mu v_{t} - \\frac{\\alpha \\delta L(w_{t})}{\\sqrt{g_{t+1} - m_{t+1}^{2} + \\epsilon}} $ </center>\n",
    "\n",
    "Updating weight,\n",
    "\n",
    "<center> $w_{t+1} = w_{t} + v_{t+1}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "Combines AdaGrad and RMSProp, uses accumulated gradient but both the first order moment $m$ and the second order moment $g$ decay over time.\n",
    "\n",
    "Calculating first order moment,\n",
    "\n",
    "<center> $\\hat{m}_{t+1} = \\frac{\\gamma_{1}m_{t} + (1-\\gamma_{1})\\bigtriangledown L(w_{t})}{1 - \\gamma_{1}^{t+1}}$ </center>\n",
    "\n",
    "\n",
    "Calculating second order moment,\n",
    "\n",
    "<center> $\\hat{g}_{t+1} = \\frac{\\gamma_{2}g_{t} + (1-\\gamma_{2})\\bigtriangledown L(w_{t})^{2}}{1 - \\gamma_{2}^{t+1}}$ </center>\n",
    "\n",
    "<center> $w_{t+1} = w_{t} - \\frac{\\alpha \\hat{m}_{t+1}}{\\sqrt{\\hat{g}_{t+1}} + \\epsilon}$</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
